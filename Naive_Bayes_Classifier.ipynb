{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee78d9ec",
   "metadata": {},
   "source": [
    "## Title: Heart Disease Prediction Using Generative Classifiers (Naive Bayes Classifiers) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64813d9",
   "metadata": {},
   "source": [
    "# **Step 1: Import Necessary Python Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f355248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import exp, sqrt, pi\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4fc9e2",
   "metadata": {},
   "source": [
    "# **Step 2: Load Dataset (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9699aba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>129.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable defect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "      <td>187.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>172.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age     sex    dataset               cp  trestbps   chol    fbs  \\\n",
       "0   1   63    Male  Cleveland   typical angina     145.0  233.0   True   \n",
       "1   2   67    Male  Cleveland     asymptomatic     160.0  286.0  False   \n",
       "2   3   67    Male  Cleveland     asymptomatic     120.0  229.0  False   \n",
       "3   4   37    Male  Cleveland      non-anginal     130.0  250.0  False   \n",
       "4   5   41  Female  Cleveland  atypical angina     130.0  204.0  False   \n",
       "\n",
       "          restecg  thalch  exang  oldpeak        slope   ca  \\\n",
       "0  lv hypertrophy   150.0  False      2.3  downsloping  0.0   \n",
       "1  lv hypertrophy   108.0   True      1.5         flat  3.0   \n",
       "2  lv hypertrophy   129.0   True      2.6         flat  2.0   \n",
       "3          normal   187.0  False      3.5  downsloping  0.0   \n",
       "4  lv hypertrophy   172.0  False      1.4    upsloping  0.0   \n",
       "\n",
       "                thal  num  \n",
       "0       fixed defect    0  \n",
       "1             normal    2  \n",
       "2  reversable defect    1  \n",
       "3             normal    0  \n",
       "4             normal    0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data from csv file placed locally in the pc\n",
    "df = pd.read_csv('heart_disease_uci.csv')\n",
    "\n",
    "# print the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b689507",
   "metadata": {},
   "source": [
    "# Step 3: Understanding the UCI Heart Disease Dataset\n",
    "\n",
    "Let's now take a look at our dataset attributes and understand their meaning and significance.\n",
    "\n",
    "\n",
    "| Attribute Name | Type | Description |\n",
    "|-----------------------|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| id | Discrete | Unique identity for each patient |\n",
    "| age | Continuous | Represents age of the patient in years|\n",
    "| sex  | Categorical | Represents male or female  <br>(1 = male, 0 = female) |\n",
    "| dataset| Categorical |   Represents the place of study <br>(0:Cleveland, 1:Hungary, 2:Switzerland, 3:VA Long Beach) |\n",
    "| cp| Categorical |   Represents the chest pain type <br>(0: asymptomatic, 1: atypical angina, 2: non-anginal pain, 3: typical angina) |\n",
    "| trestbps | Continuous | resting blood pressure  (in mm Hg on admission to the hospital) |\n",
    "| chol | Continuous | serum cholesterol  (in mg/dl) |\n",
    "| fbs  | Categorical | Represents if fasting blood sugar > 120 mg/dl <br>(0 = false, 1 = true)|\n",
    "| restecg  | Categorical | Represents the resting electrocardiographic results <br>(0: showing probable or definite left ventricular hypertrophy by Estesâ€™ criteria, 1: normal, 2: having ST-T wave abnormality)|\n",
    "| thalach | Continuous | The maximum heart rate achieved |\n",
    "| exang  | Categorical | Represents the exercise-induced angina <br>(0 = false, 1 = true)|\n",
    "| oldpeak | Continuous | ST depression induced by exercise relative to rest |\n",
    "| slope  | Categorical | Represents the  slope of the peak exercise ST segment <br>(0: downsloping; 1: flat; 2: upsloping)|\n",
    "| ca | Continuous | number of major vessels (0-3) colored by fluoroscopy |\n",
    "| thal | Categorical | Represents <br>(0 = normal, 1 = fixed defect, 2 = reversible defect) |\n",
    "| num | Discrete | Represents the class label or predicted attribute where 0 indicates no heart disease and 1, 2, 3, and 4 represent the different stages of heart disease. <br>(0,1,2,3,4) |\n",
    "\n",
    "We have a total of 14 features and our objective is to predict if the patient has a heart disease. Hence we will be building and interpreting a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21cd583a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(920, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows (sample size) and columns in the data\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e757af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 920 entries, 0 to 919\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        920 non-null    int64  \n",
      " 1   age       920 non-null    int64  \n",
      " 2   sex       920 non-null    object \n",
      " 3   dataset   920 non-null    object \n",
      " 4   cp        920 non-null    object \n",
      " 5   trestbps  861 non-null    float64\n",
      " 6   chol      890 non-null    float64\n",
      " 7   fbs       830 non-null    object \n",
      " 8   restecg   918 non-null    object \n",
      " 9   thalch    865 non-null    float64\n",
      " 10  exang     865 non-null    object \n",
      " 11  oldpeak   858 non-null    float64\n",
      " 12  slope     611 non-null    object \n",
      " 13  ca        309 non-null    float64\n",
      " 14  thal      434 non-null    object \n",
      " 15  num       920 non-null    int64  \n",
      "dtypes: float64(5), int64(3), object(8)\n",
      "memory usage: 115.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#check for data type per column \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86494152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "age           0\n",
       "sex           0\n",
       "dataset       0\n",
       "cp            0\n",
       "trestbps     59\n",
       "chol         30\n",
       "fbs          90\n",
       "restecg       2\n",
       "thalch       55\n",
       "exang        55\n",
       "oldpeak      62\n",
       "slope       309\n",
       "ca          611\n",
       "thal        486\n",
       "num           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for number of missing data type per column using either of the two functions\n",
    "#df.isna().sum()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6579b4c4",
   "metadata": {},
   "source": [
    "# **Step 4: Scale continous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5411b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to scale\n",
    "columns_to_scale = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
    "\n",
    "# Scale each column\n",
    "for column in columns_to_scale:\n",
    "    mean_value = df[column].mean()  # Calculate mean, ignoring NaNs\n",
    "    std_value = df[column].std()    # Calculate standard deviation, ignoring NaNs\n",
    "    df[column] = (df[column] - mean_value) / std_value  # Apply scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b8f235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.006838</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>0.674895</td>\n",
       "      <td>0.305736</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>0.480375</td>\n",
       "      <td>False</td>\n",
       "      <td>1.302399</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>-0.722891</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.431255</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>1.461633</td>\n",
       "      <td>0.784158</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>-1.139603</td>\n",
       "      <td>True</td>\n",
       "      <td>0.569279</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.483426</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.431255</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>-0.636335</td>\n",
       "      <td>0.269628</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>-0.329614</td>\n",
       "      <td>True</td>\n",
       "      <td>1.577319</td>\n",
       "      <td>flat</td>\n",
       "      <td>1.414653</td>\n",
       "      <td>reversable defect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.751875</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>-0.111843</td>\n",
       "      <td>0.459192</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "      <td>1.907499</td>\n",
       "      <td>False</td>\n",
       "      <td>2.402079</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>-0.722891</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-1.327458</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>-0.111843</td>\n",
       "      <td>0.043958</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>1.328935</td>\n",
       "      <td>False</td>\n",
       "      <td>0.477639</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>-0.722891</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       age     sex    dataset               cp  trestbps      chol  \\\n",
       "0   1  1.006838    Male  Cleveland   typical angina  0.674895  0.305736   \n",
       "1   2  1.431255    Male  Cleveland     asymptomatic  1.461633  0.784158   \n",
       "2   3  1.431255    Male  Cleveland     asymptomatic -0.636335  0.269628   \n",
       "3   4 -1.751875    Male  Cleveland      non-anginal -0.111843  0.459192   \n",
       "4   5 -1.327458  Female  Cleveland  atypical angina -0.111843  0.043958   \n",
       "\n",
       "     fbs         restecg    thalch  exang   oldpeak        slope        ca  \\\n",
       "0   True  lv hypertrophy  0.480375  False  1.302399  downsloping -0.722891   \n",
       "1  False  lv hypertrophy -1.139603   True  0.569279         flat  2.483426   \n",
       "2  False  lv hypertrophy -0.329614   True  1.577319         flat  1.414653   \n",
       "3  False          normal  1.907499  False  2.402079  downsloping -0.722891   \n",
       "4  False  lv hypertrophy  1.328935  False  0.477639    upsloping -0.722891   \n",
       "\n",
       "                thal  num  \n",
       "0       fixed defect    0  \n",
       "1             normal    2  \n",
       "2  reversable defect    1  \n",
       "3             normal    0  \n",
       "4             normal    0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f833a",
   "metadata": {},
   "source": [
    "# **Step 5: Compute Naive Bayes Classifiers Without Sklearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea2c27",
   "metadata": {},
   "source": [
    "**Step 5a: Shuffle and split the dataset into training and test sets without using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26fda264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle and Split the dataset\n",
    "df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into training and test sets (80:20)\n",
    "train_size = int(0.8 * len(df_shuffled))  # 80% for training\n",
    "train_set = df_shuffled[:train_size]\n",
    "test_set = df_shuffled[train_size:]\n",
    "\n",
    "# Separate features and labels \n",
    "X_train = train_set.drop(columns=['id', 'num'])  # Exclude 'id' and 'num'\n",
    "y_train = train_set['num']\n",
    "X_test = test_set.drop(columns=['num'])\n",
    "y_test = test_set['num']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c702cc",
   "metadata": {},
   "source": [
    "**Step 5b: Functions to compute statistics and probablities of Naive Bayes Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8204adf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prior Probabilities for Each Class:\n",
      "Class: 0, Prior Probability: 0.4416\n",
      "Class: 1, Prior Probability: 0.2908\n",
      "Class: 2, Prior Probability: 0.1264\n",
      "Class: 3, Prior Probability: 0.1141\n",
      "Class: 4, Prior Probability: 0.0272\n",
      "\n",
      "Conditional Probabilities Parameter for Each Continous Feature per Class:\n",
      "Class: 0\n",
      "  Feature: age, Mean: -0.3337, Std Dev: 1.0100, Count: 325\n",
      "  Feature: trestbps, Mean: -0.1429, Std Dev: 0.8441, Count: 309\n",
      "  Feature: chol, Mean: 0.2670, Std Dev: 0.6599, Count: 311\n",
      "  Feature: thalch, Mean: 0.4584, Std Dev: 0.9180, Count: 309\n",
      "  Feature: oldpeak, Mean: -0.4242, Std Dev: 0.6511, Count: 308\n",
      "  Feature: ca, Mean: -0.3940, Std Dev: 0.7391, Count: 130\n",
      "Class: 1\n",
      "  Feature: age, Mean: 0.0221, Std Dev: 0.9264, Count: 214\n",
      "  Feature: trestbps, Mean: 0.0587, Std Dev: 1.0489, Count: 202\n",
      "  Feature: chol, Mean: -0.0345, Std Dev: 1.0973, Count: 207\n",
      "  Feature: thalch, Mean: -0.2340, Std Dev: 0.9220, Count: 202\n",
      "  Feature: oldpeak, Mean: 0.0561, Std Dev: 0.9915, Count: 200\n",
      "  Feature: ca, Mean: 0.0787, Std Dev: 0.9223, Count: 48\n",
      "Class: 2\n",
      "  Feature: age, Mean: 0.3782, Std Dev: 0.8346, Count: 93\n",
      "  Feature: trestbps, Mean: 0.0087, Std Dev: 0.9134, Count: 87\n",
      "  Feature: chol, Mean: -0.5463, Std Dev: 1.2160, Count: 91\n",
      "  Feature: thalch, Mean: -0.3476, Std Dev: 0.8974, Count: 88\n",
      "  Feature: oldpeak, Mean: 0.4492, Std Dev: 1.1332, Count: 87\n",
      "  Feature: ca, Mean: 0.3827, Std Dev: 0.9248, Count: 29\n",
      "Class: 3\n",
      "  Feature: age, Mean: 0.5332, Std Dev: 0.8725, Count: 84\n",
      "  Feature: trestbps, Mean: 0.1024, Std Dev: 1.3737, Count: 71\n",
      "  Feature: chol, Mean: -0.3725, Std Dev: 1.1465, Count: 83\n",
      "  Feature: thalch, Mean: -0.6820, Std Dev: 1.0395, Count: 73\n",
      "  Feature: oldpeak, Mean: 0.5928, Std Dev: 1.1081, Count: 70\n",
      "  Feature: ca, Mean: 0.6665, Std Dev: 1.0178, Count: 30\n",
      "Class: 4\n",
      "  Feature: age, Mean: 0.5877, Std Dev: 0.7944, Count: 20\n",
      "  Feature: trestbps, Mean: 0.2491, Std Dev: 1.1585, Count: 17\n",
      "  Feature: chol, Mean: -0.1470, Std Dev: 0.9603, Count: 19\n",
      "  Feature: thalch, Mean: -0.5675, Std Dev: 0.9742, Count: 18\n",
      "  Feature: oldpeak, Mean: 1.3788, Std Dev: 1.1920, Count: 18\n",
      "  Feature: ca, Mean: 0.8209, Std Dev: 1.2081, Count: 9\n",
      "\n",
      "Conditional Probabilities for Each Categorical Feature per Class:\n",
      "Class: 0\n",
      "  Feature: sex, Probabilities: {'Male': 0.6553846153846153, 'Female': 0.3446153846153846}\n",
      "  Feature: dataset, Probabilities: {'Hungary': 0.46153846153846156, 'Cleveland': 0.39692307692307693, 'VA Long Beach': 0.12, 'Switzerland': 0.021538461538461538}\n",
      "  Feature: cp, Probabilities: {'atypical angina': 0.3630769230769231, 'non-anginal': 0.3292307692307692, 'asymptomatic': 0.24923076923076923, 'typical angina': 0.05846153846153846}\n",
      "  Feature: fbs, Probabilities: {False: 0.8821656050955414, True: 0.1178343949044586}\n",
      "  Feature: restecg, Probabilities: {'normal': 0.6553846153846153, 'lv hypertrophy': 0.19692307692307692, 'st-t abnormality': 0.1476923076923077}\n",
      "  Feature: exang, Probabilities: {False: 0.8737864077669902, True: 0.1262135922330097}\n",
      "  Feature: slope, Probabilities: {'upsloping': 0.572289156626506, 'flat': 0.3674698795180723, 'downsloping': 0.060240963855421686}\n",
      "  Feature: thal, Probabilities: {'normal': 0.738255033557047, 'reversable defect': 0.19463087248322147, 'fixed defect': 0.06711409395973154}\n",
      "Class: 1\n",
      "  Feature: sex, Probabilities: {'Male': 0.8878504672897196, 'Female': 0.11214953271028037}\n",
      "  Feature: dataset, Probabilities: {'Hungary': 0.37850467289719625, 'Cleveland': 0.21962616822429906, 'VA Long Beach': 0.21962616822429906, 'Switzerland': 0.1822429906542056}\n",
      "  Feature: cp, Probabilities: {'asymptomatic': 0.7289719626168224, 'non-anginal': 0.14953271028037382, 'atypical angina': 0.06542056074766354, 'typical angina': 0.056074766355140186}\n",
      "  Feature: fbs, Probabilities: {False: 0.8152173913043478, True: 0.18478260869565216}\n",
      "  Feature: restecg, Probabilities: {'normal': 0.6273584905660378, 'lv hypertrophy': 0.18867924528301888, 'st-t abnormality': 0.18396226415094338}\n",
      "  Feature: exang, Probabilities: {True: 0.5445544554455446, False: 0.45544554455445546}\n",
      "  Feature: slope, Probabilities: {'flat': 0.7320261437908496, 'upsloping': 0.1895424836601307, 'downsloping': 0.0784313725490196}\n",
      "  Feature: thal, Probabilities: {'reversable defect': 0.550561797752809, 'normal': 0.3146067415730337, 'fixed defect': 0.1348314606741573}\n",
      "Class: 2\n",
      "  Feature: sex, Probabilities: {'Male': 0.8924731182795699, 'Female': 0.10752688172043011}\n",
      "  Feature: dataset, Probabilities: {'VA Long Beach': 0.3548387096774194, 'Switzerland': 0.3333333333333333, 'Cleveland': 0.3118279569892473}\n",
      "  Feature: cp, Probabilities: {'asymptomatic': 0.8494623655913979, 'non-anginal': 0.10752688172043011, 'typical angina': 0.03225806451612903, 'atypical angina': 0.010752688172043012}\n",
      "  Feature: fbs, Probabilities: {False: 0.7123287671232876, True: 0.2876712328767123}\n",
      "  Feature: restecg, Probabilities: {'normal': 0.5268817204301075, 'st-t abnormality': 0.26881720430107525, 'lv hypertrophy': 0.20430107526881722}\n",
      "  Feature: exang, Probabilities: {True: 0.5568181818181818, False: 0.4431818181818182}\n",
      "  Feature: slope, Probabilities: {'flat': 0.6103896103896104, 'upsloping': 0.2077922077922078, 'downsloping': 0.18181818181818182}\n",
      "  Feature: thal, Probabilities: {'reversable defect': 0.6226415094339622, 'normal': 0.18867924528301888, 'fixed defect': 0.18867924528301888}\n",
      "Class: 3\n",
      "  Feature: sex, Probabilities: {'Male': 0.9285714285714286, 'Female': 0.07142857142857142}\n",
      "  Feature: dataset, Probabilities: {'VA Long Beach': 0.38095238095238093, 'Cleveland': 0.34523809523809523, 'Switzerland': 0.27380952380952384}\n",
      "  Feature: cp, Probabilities: {'asymptomatic': 0.7738095238095238, 'non-anginal': 0.16666666666666666, 'atypical angina': 0.03571428571428571, 'typical angina': 0.023809523809523808}\n",
      "  Feature: fbs, Probabilities: {False: 0.72, True: 0.28}\n",
      "  Feature: restecg, Probabilities: {'normal': 0.4642857142857143, 'lv hypertrophy': 0.27380952380952384, 'st-t abnormality': 0.2619047619047619}\n",
      "  Feature: exang, Probabilities: {True: 0.6438356164383562, False: 0.3561643835616438}\n",
      "  Feature: slope, Probabilities: {'flat': 0.6212121212121212, 'upsloping': 0.21212121212121213, 'downsloping': 0.16666666666666666}\n",
      "  Feature: thal, Probabilities: {'reversable defect': 0.6730769230769231, 'normal': 0.21153846153846154, 'fixed defect': 0.11538461538461539}\n",
      "Class: 4\n",
      "  Feature: sex, Probabilities: {'Male': 1.0}\n",
      "  Feature: dataset, Probabilities: {'Cleveland': 0.45, 'VA Long Beach': 0.4, 'Switzerland': 0.15}\n",
      "  Feature: cp, Probabilities: {'asymptomatic': 0.75, 'non-anginal': 0.2, 'typical angina': 0.05}\n",
      "  Feature: fbs, Probabilities: {False: 0.7894736842105263, True: 0.21052631578947367}\n",
      "  Feature: restecg, Probabilities: {'lv hypertrophy': 0.4, 'st-t abnormality': 0.35, 'normal': 0.25}\n",
      "  Feature: exang, Probabilities: {True: 0.6111111111111112, False: 0.3888888888888889}\n",
      "  Feature: slope, Probabilities: {'flat': 0.625, 'downsloping': 0.3125, 'upsloping': 0.0625}\n",
      "  Feature: thal, Probabilities: {'reversable defect': 0.6363636363636364, 'fixed defect': 0.2727272727272727, 'normal': 0.09090909090909091}\n",
      "\n",
      "Posterior Probabilities for the Example Data Point:\n",
      "Class: 0, Posterior Probability: 0.0000\n",
      "Class: 1, Posterior Probability: 0.1467\n",
      "Class: 2, Posterior Probability: 0.3460\n",
      "Class: 3, Posterior Probability: 0.4846\n",
      "Class: 4, Posterior Probability: 0.0227\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the mean of a list of numbers, ignoring NaN values\n",
    "def calculate_mean(numbers):\n",
    "    numbers = [x for x in numbers if pd.notna(x)]  # Filter out NaN values\n",
    "    if len(numbers) == 0:\n",
    "        return float('nan')  # Return NaN if the list is empty\n",
    "    return sum(numbers) / float(len(numbers))\n",
    "\n",
    "# Function to calculate the standard deviation of a list of numbers, ignoring NaN values\n",
    "def calculate_stdev(numbers):\n",
    "    numbers = [x for x in numbers if pd.notna(x)]  # Filter out NaN values\n",
    "    if len(numbers) < 2:\n",
    "        return float('nan')  # Return NaN for stdev if there are less than 2 valid numbers\n",
    "    avg = calculate_mean(numbers)\n",
    "    variance = sum([(x - avg) ** 2 for x in numbers]) / float(len(numbers) - 1)\n",
    "    return variance ** 0.5  # Return the standard deviation\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess_data(train_set, target_column='num'):\n",
    "    X_train = train_set.drop(columns=['id', target_column])  # Drop irrelevant columns\n",
    "    y_train = train_set[target_column]  \n",
    "    return X_train, y_train\n",
    "\n",
    "# Function to summarize statistics by class\n",
    "def summarize_class_statistics(X_train, y_train, feature_set):\n",
    "    class_summaries = {}\n",
    "    \n",
    "    for class_value in np.unique(y_train):\n",
    "        class_data = X_train[y_train == class_value]\n",
    "        summaries = []\n",
    "        \n",
    "        for column in feature_set:\n",
    "            column_data = class_data[column].dropna()  # Ignore missing values\n",
    "            summaries.append((column, calculate_mean(column_data), calculate_stdev(column_data), len(column_data)))\n",
    "        \n",
    "        class_summaries[class_value] = summaries\n",
    "    \n",
    "    return class_summaries\n",
    "\n",
    "# Function to calculate the Gaussian probability density function\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    if stdev == 0:  # Avoid division by zero\n",
    "        return 0\n",
    "    exponent = exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "\n",
    "def calculate_categorical_probabilities(X_train, y_train, feature_set):\n",
    "    class_probabilities = {}\n",
    "    \n",
    "    for class_value in np.unique(y_train):\n",
    "        class_data = X_train[y_train == class_value]\n",
    "        probabilities = {}\n",
    "        \n",
    "        for column in feature_set:\n",
    "            # Drop NaN values before counting\n",
    "            category_counts = class_data[column].dropna().value_counts(normalize=True)  # Normalize to get probabilities\n",
    "            probabilities[column] = category_counts.to_dict()  # Convert to dictionary format\n",
    "        \n",
    "        class_probabilities[class_value] = probabilities\n",
    "    \n",
    "    return class_probabilities\n",
    "\n",
    "\n",
    "def calculate_posterior_probabilities(X_train, y_train, example_data_point, class_summaries, class_probabilities):\n",
    "    posterior_probabilities = {}\n",
    "    \n",
    "    for class_value, summaries in class_summaries.items():\n",
    "        prior_probability = len(X_train[y_train == class_value]) / len(X_train)\n",
    "        likelihood = 1  # Start with 1 since it's a product\n",
    "        \n",
    "        # Calculate likelihood for continuous features\n",
    "        for feature_name, mean_val, stdev_val, _ in summaries:\n",
    "            x = example_data_point[feature_name]\n",
    "            if pd.notna(x):  # Check if x is not NaN\n",
    "                likelihood *= calculate_probability(x, mean_val, stdev_val)\n",
    "        \n",
    "        # Calculate likelihood for categorical features\n",
    "        for column in class_probabilities[class_value]:\n",
    "            category = example_data_point[column]\n",
    "            if pd.notna(category):  # Check if category is not NaN\n",
    "                category_probability = class_probabilities[class_value].get(column, {}).get(category, 0)\n",
    "                likelihood *= category_probability\n",
    "\n",
    "        posterior_probability = prior_probability * likelihood\n",
    "        posterior_probabilities[class_value] = posterior_probability\n",
    "    \n",
    "    # Normalize posterior probabilities\n",
    "    total_posterior = sum(posterior_probabilities.values())\n",
    "    for class_value in posterior_probabilities:\n",
    "        posterior_probabilities[class_value] /= total_posterior\n",
    "    \n",
    "    return posterior_probabilities\n",
    "\n",
    "\n",
    "    # Normalize posterior probabilities\n",
    "    total_posterior = sum(posterior_probabilities.values())\n",
    "    for class_value in posterior_probabilities:\n",
    "        posterior_probabilities[class_value] /= total_posterior\n",
    "    \n",
    "    return posterior_probabilities\n",
    "\n",
    "# Example usage\n",
    "train_set = df_shuffled[:train_size]  # Load your DataFrame\n",
    "feature_set_1 = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
    "feature_set_2 = ['sex', 'dataset', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "# Preprocess the dataset\n",
    "X_train, y_train = preprocess_data(train_set)\n",
    "\n",
    "# Summarize statistics\n",
    "class_summaries = summarize_class_statistics(X_train, y_train, feature_set_1)\n",
    "\n",
    "# Calculate categorical probabilities\n",
    "class_probabilities = calculate_categorical_probabilities(X_train, y_train, feature_set_2)\n",
    "\n",
    "# Evaluate for an example data point\n",
    "example_data_point = X_test.iloc[5]  # Select the third data point\n",
    "#print(example_data_point)\n",
    "posterior_probabilities = calculate_posterior_probabilities(X_train, y_train, example_data_point, class_summaries, class_probabilities)\n",
    "\n",
    "# Print prior probabilities for the classes\n",
    "prior_probabilities = {class_value: len(X_train[y_train == class_value]) / len(X_train) for class_value in np.unique(y_train)}\n",
    "print(\"\\nPrior Probabilities for Each Class:\")\n",
    "for class_value, prob in prior_probabilities.items():\n",
    "    print(f'Class: {class_value}, Prior Probability: {prob:.4f}')\n",
    "\n",
    "# Print conditional probabilities for each feature per class\n",
    "print(\"\\nConditional Probabilities Parameter for Each Continous Feature per Class:\")\n",
    "for class_value, summaries in class_summaries.items():\n",
    "    print(f'Class: {class_value}')\n",
    "    for feature_name, mean_val, stdev_val, count in summaries:\n",
    "        print(f'  Feature: {feature_name}, Mean: {mean_val:.4f}, Std Dev: {stdev_val:.4f}, Count: {count}')\n",
    "\n",
    "# Print categorical probabilities for each class\n",
    "print(\"\\nConditional Probabilities for Each Categorical Feature per Class:\")\n",
    "for class_value, probabilities in class_probabilities.items():\n",
    "    print(f'Class: {class_value}')\n",
    "    for feature_name, feature_probs in probabilities.items():\n",
    "        print(f'  Feature: {feature_name}, Probabilities: {feature_probs}')\n",
    "\n",
    "# Print posterior probabilities for the example data point\n",
    "print(\"\\nPosterior Probabilities for the Example Data Point:\")\n",
    "for class_value, prob in posterior_probabilities.items():\n",
    "    print(f'Class: {class_value}, Posterior Probability: {prob:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aae455",
   "metadata": {},
   "source": [
    "**Step 5c: Functions to compute the confusion matrix for Naive Bayes Classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b032b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Precision: 0.86, Recall: 0.74, F1 Score: 0.80\n",
      "Class 1 - Precision: 0.51, Recall: 0.57, F1 Score: 0.54\n",
      "Class 2 - Precision: 0.12, Recall: 0.19, F1 Score: 0.15\n",
      "Class 3 - Precision: 0.29, Recall: 0.30, F1 Score: 0.30\n",
      "Class 4 - Precision: 0.20, Recall: 0.12, F1 Score: 0.15\n",
      "  \n",
      "Overall Accuracy on the test set: 0.83\n",
      "Average Precision: 0.40\n",
      "Average Recall: 0.39\n",
      "Average F1 Score: 0.39\n",
      "Average F1 Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the class with the highest posterior probability\n",
    "def predict_class_nb(X_train, y_train, example_data_point, class_summaries, class_probabilities):\n",
    "    posterior_probabilities = calculate_posterior_probabilities(X_train, y_train, example_data_point, class_summaries, class_probabilities)\n",
    "    best_class = max(posterior_probabilities, key=posterior_probabilities.get)\n",
    "    return best_class\n",
    "\n",
    "# Function to compute confusion matrix for multi-class classification\n",
    "def compute_confusion_matrix(X_test, y_test, X_train, y_train, class_summaries, class_probabilities):\n",
    "    # Initialize counts for True Positives, True Negatives, False Positives, and False Negatives\n",
    "    True_positives = [0] * 5\n",
    "    True_negatives = [0] * 5\n",
    "    False_positives = [0] * 5\n",
    "    False_negatives = [0] * 5\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        example_data_point = X_test.iloc[i]\n",
    "        actual_class = y_test.iloc[i]\n",
    "        predicted_class = predict_class_nb(X_train, y_train, example_data_point, class_summaries, class_probabilities)\n",
    "\n",
    "        # Confusion matrix based on predicted and actual classes\n",
    "        if predicted_class == actual_class:\n",
    "            True_positives[actual_class] += 1\n",
    "        else:\n",
    "            False_positives[predicted_class] += 1\n",
    "            False_negatives[actual_class] += 1\n",
    "\n",
    "    # Compute True Negatives for each class\n",
    "    for j in range(5):\n",
    "        True_negatives[j] = len(X_test) - (True_positives[j] + False_positives[j] + False_negatives[j])\n",
    "\n",
    "    # Calculate performance metrics for each class\n",
    "    class_specific_accuracy = [(True_positives[j] + True_negatives[j]) / (True_positives[j] + True_negatives[j] + False_positives[j] + False_negatives[j]) if (True_positives[j] + True_negatives[j] + False_positives[j] + False_negatives[j]) > 0 else 0 for j in range(5)]\n",
    "    class_specific_precision = [(True_positives[j]) / (True_positives[j] + False_positives[j]) if (True_positives[j] + False_positives[j]) > 0 else 0 for j in range(5)]\n",
    "    class_specific_recall = [(True_positives[j]) / (True_positives[j] + False_negatives[j]) if (True_positives[j] + False_negatives[j]) > 0 else 0 for j in range(5)]\n",
    "    class_specific_FScore = [(2 * class_specific_precision[j] * class_specific_recall[j]) / (class_specific_precision[j] + class_specific_recall[j]) if (class_specific_precision[j] + class_specific_recall[j]) > 0 else 0 for j in range(5)]\n",
    "\n",
    "    # Compute overall model performance\n",
    "    Average_accuracy = sum(class_specific_accuracy) / 5\n",
    "    Average_precision = sum(class_specific_precision) / 5\n",
    "    Average_recall = sum(class_specific_recall) / 5\n",
    "    Average_FScore = sum(class_specific_FScore) / 5\n",
    "\n",
    "    return Average_accuracy, Average_precision, Average_recall, Average_FScore, class_specific_accuracy, class_specific_precision, class_specific_recall, class_specific_FScore\n",
    "\n",
    "\n",
    "# Split the test set\n",
    "test_set = df_shuffled[train_size:]\n",
    "X_test, y_test = preprocess_data(test_set)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "Average_accuracy, Average_precision, Average_recall, Average_FScore, class_specific_accuracy, class_specific_precision, class_specific_recall, class_specific_FScore = compute_confusion_matrix(X_test, y_test, X_train, y_train, class_summaries, class_probabilities)\n",
    "\n",
    "# Print Precision, Recall, and F1 Score for each class\n",
    "for i in range(5):\n",
    "    print(f'Class {i} - Precision: {class_specific_precision[i]:.2f}, Recall: {class_specific_recall[i]:.2f}, F1 Score: {class_specific_FScore[i]:.2f}')\n",
    "print(\"  \")\n",
    "    # Print the overall accuracy\n",
    "print(f'Overall Accuracy on the test set: {Average_accuracy:.2f}')\n",
    "\n",
    "# Print Average Precision, Recall, and F1 Score\n",
    "print(f'Average Precision: {Average_precision:.2f}')\n",
    "print(f'Average Recall: {Average_recall:.2f}')\n",
    "print(f'Average F1 Score: {Average_FScore:.2f}')\n",
    "print(f'Average F1 Score: {Average_accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
